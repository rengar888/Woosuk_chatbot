from flask import Flask, request, jsonify
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import pipeline # LLM ìƒì„± íŒŒì´í”„ë¼ì¸
import os

app = Flask(__name__)

# --- ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì • ---
# í•™ì‚¬ ì •ë³´ ì²­í¬ ë°ì´í„°ë¥¼ ì—¬ê¸°ì— ì‚½ì…
DOCUMENT_CHUNKS = [
    "ìš°ì„ëŒ€í•™êµì˜ 2025í•™ë…„ë„ 1í•™ê¸° ë“±ë¡ê¸ˆ ë‚©ë¶€ ê¸°ê°„ì€ 2ì›” 20ì¼ë¶€í„° 2ì›” 24ì¼ê¹Œì§€ì…ë‹ˆë‹¤.",
    "ë“±ë¡ê¸ˆì„ ë¯¸ë‚©í•  ê²½ìš° í•™ì  ì·¨ì†Œ ë° ì œì  ì²˜ë¦¬ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ê¸°í•œ ë‚´ ë‚©ë¶€í•´ì•¼ í•©ë‹ˆë‹¤.",
    
    # ğŸ’¡ [í•™ì‹ ë©”ë‰´ ì •ë³´ ì¶”ê°€]
    "í•™ì‹ ë©”ë‰´ëŠ” êµë‚´ ì‹ë‹¹ì—ì„œ ì œê³µë˜ë©°, ì´ìš© ì‹œê°„ì€ **ì ì‹¬ì€ 11ì‹œ 30ë¶„ë¶€í„° 13ì‹œ 30ë¶„ê¹Œì§€**, **ì €ë…ì€ 17ì‹œë¶€í„° 18ì‹œ 30ë¶„ê¹Œì§€**ì…ë‹ˆë‹¤. ì‹ë‹¹ë³„ ìš´ì˜ ì‹œê°„ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    
    # ğŸ’¡ [ìˆ˜ê°•ì‹ ì²­/ì¡¸ì—…ì‚¬ì •íšŒ ì •ë³´ êµ¬ì²´í™”]
    "2024í•™ë…„ë„ 1í•™ê¸° ìˆ˜ê°•ì‹ ì²­ ì•ˆë‚´ ë° ì¡¸ì—… ì‚¬ì •íšŒëŠ” **2024ë…„ 11ì›” 10ì¼**ì— ê°œìµœë  ì˜ˆì •ì´ë©°, ìˆ˜ê°•ì‹ ì²­ì€ 11ì›” 15ì¼ë¶€í„° 17ì¼ê¹Œì§€ ì§„í–‰ë©ë‹ˆë‹¤.",
    
    "ì…”í‹€ë²„ìŠ¤ëŠ” ì‚¼ë¡€ìº í¼ìŠ¤ì™€ ì „ì£¼ì—­, ìµì‚°ì—­ì„ ìš´í–‰í•˜ë©° ìì„¸í•œ ì‹œê°„í‘œëŠ” í•™êµ í¬í„¸ ê³µì§€ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”."
]

MODEL = None       # Sentence Transformer (ê²€ìƒ‰ìš©)
FAISS_INDEX = None # Faiss ì¸ë±ìŠ¤
GENERATOR = None   # LLM ìƒì„± íŒŒì´í”„ë¼ì¸ (ìƒì„±ìš©)
VECTOR_DIMENSION = 0

def retrieve_relevant_context(query, index, model, chunks, k=2):
    """Faiss ì¸ë±ìŠ¤ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ë§¥ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query_embedding = model.encode([query], convert_to_numpy=True)
    D, I = index.search(query_embedding, k) 
    retrieved_chunks = [chunks[i] for i in I[0]]
    return retrieved_chunks

def initialize_search_engine():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ëª¨ë¸ê³¼ ì¸ë±ìŠ¤ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global MODEL, FAISS_INDEX, VECTOR_DIMENSION, GENERATOR
    
    try:
        # 1. ê²€ìƒ‰ ëª¨ë¸ (Sentence Transformer) ë¡œë“œ ë° Faiss ì¸ë±ìŠ¤ êµ¬ì¶•
        print("1. ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì¤‘...")
        model_name = 'sentence-transformers/all-MiniLM-L6-v2' 
        MODEL = SentenceTransformer(model_name)
        document_embeddings = MODEL.encode(DOCUMENT_CHUNKS, convert_to_numpy=True)
        VECTOR_DIMENSION = document_embeddings.shape[1]
        FAISS_INDEX = faiss.IndexFlatIP(VECTOR_DIMENSION)
        FAISS_INDEX.add(document_embeddings)
        
        # 2. LLM ìƒì„± ëª¨ë¸ ë¡œë“œ (GPUê°€ ì—†ë‹¤ë©´ CPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
        print("2. LLM ìƒì„± ëª¨ë¸ ë¡œë“œ ì¤‘ (ìì› ì†Œëª¨ í¼)...")
        # âš ï¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì˜ˆì‹œ ëª¨ë¸ì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš© ì‹œ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        # CPU ì‚¬ìš© ì‹œ device=-1
        GENERATOR = pipeline(
            "text-generation", 
            model="skt/kogpt2-base-v2", 
            device=-1 
        )
        print("âœ… ëª¨ë“  ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False

# --- ìµœì¢… ì±—ë´‡ API ì—”ë“œí¬ì¸íŠ¸ ---
@app.route('/chat', methods=['POST'])
def get_chatbot_response():
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ê²€ìƒ‰ -> ìƒì„±ê¹Œì§€ ëª¨ë‘ ìˆ˜í–‰í•˜ê³  ìµœì¢… ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not GENERATOR:
        return jsonify({"error": "LLM ìƒì„± ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}), 503
        
    data = request.get_json()
    query = data.get('query')
    if not query:
        return jsonify({"error": "ê²€ìƒ‰ì–´(query)ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤."}), 400
        
    try:
        # 1. (ê²€ìƒ‰): Faissë¥¼ ì´ìš©í•´ ê´€ë ¨ ë¬¸ë§¥(context) ê²€ìƒ‰
        retrieved_chunks = retrieve_relevant_context(query, FAISS_INDEX, MODEL, DOCUMENT_CHUNKS, k=4)
        context = "\n".join(retrieved_chunks)

        # 2. (í”„ë¡¬í”„íŠ¸ êµ¬ì„±)
        system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ìš°ì„ëŒ€í•™êµ í•™ì‚¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ì°¸ê³  ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
        rag_prompt = f"{system_prompt}\n\n--- ì°¸ê³  ë¬¸ë§¥ ---\n{context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {query}\n\në‹µë³€:"

        # 3. (ìƒì„±): ë¡œì»¬ LLM í˜¸ì¶œ
        response = GENERATOR(
            rag_prompt, 
            max_length=256, 
            num_return_sequences=1, 
            do_sample=True, 
            top_p=0.9,
            repetition_penalty=1.2,
            pad_token_id=GENERATOR.tokenizer.eos_token_id # í…ìŠ¤íŠ¸ ìƒì„± ì¢…ë£Œ ì²˜ë¦¬
        )
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ (ëª¨ë¸ì— ë”°ë¼ íŒŒì‹±ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ)
        full_text = response[0]['generated_text']
        final_answer = full_text.split("ë‹µë³€:")[-1].strip()
        
        return jsonify({"response": final_answer})
        
    except Exception as e:
        print(f"LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return jsonify({"error": "íŒŒì´ì¬ ì„œë²„ì—ì„œ ë‹µë³€ ìƒì„± ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500


if __name__ == '__main__':
    # ì„œë²„ ì‹œì‘ ì „ì— ì—”ì§„ì„ ì´ˆê¸°í™”
    if initialize_search_engine():
        print("--- íŒŒì´ì¬ LLM ì„œë²„ ì‹œì‘ ---")
        # ë¦¬ëˆ…ìŠ¤ì—ì„œ ì™¸ë¶€ ì ‘ê·¼ì„ ìœ„í•´ host='0.0.0.0', í¬íŠ¸ëŠ” 8000ë²ˆ
        app.run(host='0.0.0.0', port=8000, debug=False)